---
title: "Fitting delta models with sdmTMB"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fitting delta models with sdmTMB}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache=FALSE}
dplyr_installed <- require("dplyr", quietly = TRUE)
ggplot_installed <- require("ggplot2", quietly = TRUE)
inla_installed <- requireNamespace("INLA", quietly = TRUE)
pkgs <- dplyr_installed && ggplot_installed && inla_installed
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.asp = 0.618,
  eval = identical(Sys.getenv("NOT_CRAN"), "true") && pkgs
)
```

```{r packages, message=FALSE, warning=TRUE}
library(ggplot2)
library(dplyr)
library(sdmTMB)
```

sdmTMB does not have built-in hurdle models (also called delta models), but we can fit the two components separately and easily combine the predictions. 
In this case, our delta-gamma model involves fitting a binomial presence-absence model (`family = binomial(link = "logit")`) and then a model for the positive catches only with a gamma observation distribution and a log link (`Gamma(link = "log")`).
We could also use a `lognormal(link = "log")` family.
Hurdle models are more appropriate than something like a Tweedie when there are differences in the processes controlling presence vs. abundance, or when greater flexibility to account for overdispersion is required.
A similar strategy can also be used for zero-inflated count data but with a `truncated_nbinom1(link = "log")` or `truncated_nbinom2(link = "log")` distribution instead of the gamma for the positive component.

We will use a dataset built into the sdmTMB package: trawl survey data for Pacific Cod in Queen Charlotte Sound, British Columbia, Canada. The density units are kg/km^2^. Here, X and Y are coordinates in UTM zone 9.

```{r glimpse-pcod}
glimpse(pcod)
```


```{r spde, fig.asp=0.7}
mesh1 <- make_mesh(pcod, c("X", "Y"), cutoff = 10)
```

It is not necessary to use the same mesh for both models, but one can do so by updating the first mesh to match the reduced data frame as shown here:

```{r, fig.asp=0.9, cache=FALSE}
dat2 <- subset(pcod, density > 0)
mesh2 <- make_mesh(dat2,
  xy_cols = c("X", "Y"),
  mesh = mesh1$mesh
)
plot(mesh2)
```

This delta-gamma model is similar to the Tweedie model in the [Intro to modelling with sdmTMB](https://pbs-assess.github.io/sdmTMB/articles/basic-intro.html) vignette, except that we will use `s()` for the depth effect. 

```{r model1, cache=TRUE, warning=FALSE}
m1 <- sdmTMB(
  formula = present ~ 0 + as.factor(year) + s(depth, k = 3),
  data = pcod,
  mesh = mesh1,
  time = "year", family = binomial(link = "logit"),
  spatiotemporal = "iid",
  spatial = "on"
)
m1
```

One can use different covariates in each model, but in this case we will just let the depth effect be more wiggly by not specifying `k = 3`. 

```{r model2, cache=TRUE, warning=FALSE}
m2 <- sdmTMB(
  formula = density ~ 0 + as.factor(year) + s(depth),
  data = dat2,
  mesh = mesh2,
  time = "year",
  family = Gamma(link = "log"),
  spatiotemporal = "iid",
  spatial = "on"
)
m2
```

We can inspect the effect of the smoothed covariate using `plot_smooth()`.

```{r model1-depth, cache=TRUE}
s1 <- plot_smooth(m1, ggplot = TRUE)
s1 + xlim(min(pcod$depth), max(pcod$depth))
```

```{r model2-depth, cache=TRUE}
s2 <- plot_smooth(m2, ggplot = TRUE)
s2 + xlim(min(pcod$depth), max(pcod$depth))
```

Next, we need some way of combining the predictions across the two models. 
If all we need are point predictions, we can just multiply the predictions from the two models after applying the inverse link:

```{r delta-pred-simple, echo=TRUE}
pred <- qcs_grid # use the grid as template for saving our predictions
p_bin <- predict(m1, newdata = qcs_grid)
p_pos <- predict(m2, newdata = qcs_grid)
p_bin_prob <- m1$family$linkinv(p_bin$est)
p_pos_exp <- m2$family$linkinv(p_pos$est)
pred$est_exp <- p_bin_prob * p_pos_exp
```

But if a measure of uncertainty is required, we can simulate from the joint parameter precision matrix using the `predict()` function with any number of simulations selected (e.g., `sims = 500`). 
Because the predictions come from simulated draws from the parameter covariance matrix, the predictions will become more consistent with a larger number of draws. 
However, a greater number of draws takes longer to calculate and will use more memory (larger matrix), so fewer draws (~100) may be fine for experimentation. 
A larger number (say ~1000) may be appropriate for final model runs.

```{r delta-pred-sim, echo=TRUE}
set.seed(28239)
p_bin_sim <- predict(m1, newdata = qcs_grid, sims = 500)
p_pos_sim <- predict(m2, newdata = qcs_grid, sims = 500)
p_bin_prob_sim <- m1$family$linkinv(p_bin_sim)
p_pos_exp_sim <- m2$family$linkinv(p_pos_sim)
p_combined_sim <- p_bin_prob_sim * p_pos_exp_sim
```

`p_combined_sim` is just a matrix with a row for each row of data that was predicted on and width sims (`dim(p_combined_sim)`: `r dim(p_combined_sim)`). 
You can process this matrix however you would like. 
We can save median predictions and upper and lower 95% confidence intervals:

```{r, fig.width=5}
pred$median <- apply(p_combined_sim, 1, median)
# pred$lwr <- apply(p_combined_sim, 1, quantile, probs = 0.025)
# pred$upr <- apply(p_combined_sim, 1, quantile, probs = 0.975)
plot(pred$est_exp, pred$median)
```

```{r}
ggplot(subset(pred, year == 2017), aes(X, Y, fill = median)) +
  geom_raster() +
  coord_fixed() +
  scale_fill_viridis_c(trans = "sqrt")
```

And we can now calculate spatial uncertainty:

```{r cv}
pred$cv <- apply(p_combined_sim, 1, function(x) sd(x) / mean(x))
ggplot(subset(pred, year == 2017), aes(X, Y, fill = cv)) + # 2017 as an example
  geom_raster() +
  coord_fixed() +
  scale_fill_viridis_c(trans = "log10")
```

sdmTMB also has a function for calculating an index from those draws `get_index_sims()`.
This function is just summing biomass or abundance across grid cells for each simulation draw and for each year and then calculating quantiles on the distribution of samples.
The default for this function expects the simulations to still be in log space, so we either need to `log(p_combined)`, or we can set `agg_function = function(x) sum(x)`. 

```{r get_index}
qcs_grid$area <- 4 # all 2 x 2km
ind <- get_index_sims(p_combined_sim / 1000, # convert from kg to tonnes
  agg_function = function(x) sum(x),
  area = qcs_grid$area
)
ggplot(ind, aes(year, est)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  ylab("Biomass (t)")
```

For more on modelling for the purposes of creating an index see the vignette on [Index standardization with sdmTMB](https://pbs-assess.github.io/sdmTMB/articles/index-standardization.html).
